{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Домашнее задание №2: Сравнение тэггеров"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Делаем соревнование тэггеров: с помощью трёх библиотек размечаем корпус слов и сравниваем с собственноручно размеченным Золотым Стандартом."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для этого нам понадобятся следующие библиотеки:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import csv\n",
    "import time\n",
    "import re\n",
    "from string import punctuation\n",
    "import conllu\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from ufal.udpipe import Model, Pipeline  # pylint: disable=no-name-in-module\n",
    "from pymorphy2 import MorphAnalyzer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Разметка текста"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прочтём файл с текстом, который мы взяли из интернета. Токенизируем его с помощью nltk и запишем в удобный формат для разметки:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "with open ('text.txt', encoding='utf-8') as text:\n",
    "    text = text.read()\n",
    "\n",
    "text = text.replace('…', '')\n",
    "\n",
    "list_tokens = word_tokenize(text)\n",
    "\n",
    "print(list_tokens[:10])\n",
    "print('Всего токенов:', len(list_tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Первое', 'что', 'хочется', 'сказать', ',', 'так', 'это', 'то', ',', 'что']\n",
      "Всего токенов: 365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_csv_rows = [['word', 'POS-tag']]\n",
    "\n",
    "for token in list_tokens:\n",
    "    list_row = [token, '']\n",
    "    list_csv_rows.append(list_row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('corpora.csv', \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    for line in list_csv_rows:\n",
    "        writer.writerow(line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. nltk, в принципе, токенизирует текст достаточно хорошо, сохраняя нужную пунктуацию и не разрывая слова в ненужных местах.\n",
    "2. Убирать пунктуацию нет смысла, так как все используемые нами библиотеки (о них далее) умеют разбираться с пунктуацией. Делать это разве что можно для того, чтобы сократить усилия на разметку текста."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Файлы с текстом (``text.txt``) и размеченным корпусом (``corpora.csv``) можно найти в репозитории*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обзор библиотек"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В данном домашнем задании мы будем сравнивать три библиотеки: ``udpipe``, ``stanza`` и ``pymorphy2``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``udpipe``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "``udpipe`` - это библиотека от Института Формальной и Прикладной Лингвистики (Карлов Университет, Чешская Республика). Библиотека, используя модели для языков, токенизирует, лемматизирует, ставит морфологические теги, а также строит деревья зависимостей слов в предложениях. Используется CoNLL-U формат для выдачи результатов. Видимо, не смотрит на контекст(?) Морфологические теги - это теги разметки Universal Dependencies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``stanza``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Библиотека от Stanford NLP Group (команда разработчиков и исследователей компьютерной лингвистики в Университете Стэнфорда, США). Используется нейронный пайплайн, основанный на модели языка. Так же может токенизировать (multi-word tokenization), лемматизировать, ставить морфологические теги и теги зависимостей. ``stanza`` также имеет инстурменты для выделения именованных сущностей. POS-теги основаны на разметке UD."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``pymorphy2``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Морфологический анализатор для русского и украинского языка от Михаила Коробова. С помощью словаря OpenCorpora умеет слова и отдавать все возможные их разборы, для русского языка выдавая так же вероятность правильности такого разбора. Отдаёт также нормальную форму, умеет склонять и согласовывать по числу. Не смотрит на контекст. Формат POS-тегов такой же, как в OpenCorpora."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### О токенизации"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для ``pymorphy2`` можно использовать токенизатор ``nltk``. ``udpipe`` имеет собственный токенизатор, но раз он так же не смотрит на контекст, токенизатор можно использовать и другой. ``stanza`` предполагает использование собственного токенизатора для разметки по морфологическим тегам."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В связи с вышесказанным выделим __два итога__:\n",
    "* Для разметки Золотого Стандарта возьмём набор UD. Почему? 1) Достаточно распространённый набор тегов. 2) Охватывает достаточно релевантных для русского языка частей речи (хотя вводные слова?). 3) Используется 2/3 библиотек, которые мы сравниваем. 4) Достаточно объёмный, использует разделение на более мелкие классы слов (например, ADV vs. ADV, INTJ, PART). Хотя есть и некоторые неясности: не всегда UD понимает теги и части речи так же, как они понимаются, например, в школьной традиции (первый, второй - это порядковые числительные, тогда как UD разметка даёт тег ADJ - прилагательное)\n",
    "* С токенизацией есть проблема. Золотой Стандарт был токенизирован с помощью nltk, pymorphy2 так же в нашем случае будет работать от стороннего токенизатора. Остальные библиотеки имеют свои токенизаторы. Возможно, какой-то из токенизаторов работает по другим правилам. Как быть? Оставить как есть или в каждый морфологический анализатор подавать ранее токенизированный текст? Остановимся на втором варианте."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Разметка парсеров"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прогоняем наш текст через парсеры:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``udpipe``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# качаем модель для русского\n",
    "name = 'rus_udpipe_model'\n",
    "!wget -O {rus_udpipe} https://github.com/jwijffels/udpipe.models.ud.2.0/blob/master/inst/udpipe-ud-2.0-170801/russian-ud-2.0-170801.udpipe?raw=true"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# или пользуем уже скачанную\n",
    "rus_udpipe = 'russian-ud-2.0-170801.udpipe'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "rus_model = Model.load(rus_udpipe)\n",
    "rus_pipeline = Pipeline(rus_model, 'generic_tokenizer', '', '', '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "toks_tags_text = rus_pipeline.process(text)\n",
    "print(toks_tags_text[:1000])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# newdoc\n",
      "# newpar\n",
      "# sent_id = 1\n",
      "# text = Первое что хочется сказать, так это то, что это очень необычный парк США.\n",
      "1\tПервое\tПЕРВЫЙ\tADJ\tORD\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t3\tamod\t_\t_\n",
      "2\tчто\tЧТО\tPRON\tWP\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t3\tnsubj\t_\t_\n",
      "3\tхочется\tхочется\tVERB\tVBC\tAspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Mid\t0\troot\t_\t_\n",
      "4\tсказать\tсказать\tVERB\tVB\tAspect=Imp|VerbForm=Inf\t3\txcomp\t_\tSpaceAfter=No\n",
      "5\t,\t,\tPUNCT\t,\t_\t8\tpunct\t_\t_\n",
      "6\tтак\tТАК\tSCONJ\tIN\t_\t8\tmark\t_\t_\n",
      "7\tэто\tЭТО\tPRON\tDT\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t8\tnsubj\t_\t_\n",
      "8\tто\tТО\tPRON\tDT\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t3\tadvcl\t_\tSpaceAfter=No\n",
      "9\t,\t,\tPUNCT\t,\t_\t13\tpunct\t_\t_\n",
      "10\tчто\tЧТО\tSCONJ\tIN\t_\t13\tmark\t_\t_\n",
      "11\tэто\tЭТО\tPRON\tDT\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t13\tnsubj\t_\t_\n",
      "12\tочень\tОЧЕНЬ\tADV\tRB\t_\t13\tadvmod\t_\t_\n",
      "13\tнеобычный\tНЕОБЫЧНЫЙ\tADJ\tJJL\tAnimacy=Inan|Case=Acc|Gender=Masc|Number=Sing\t8\tccomp\t_\t_\n",
      "14\tпарк\tПАРК\tNOUN\tNN\tAnimacy=Inan|Case=Acc|Gender=M\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# распарсим CoNLL-U формат\n",
    "final_parse = conllu.parse(toks_tags_text)\n",
    "final_parse[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[TokenList<Первое, что, хочется, сказать, ,, так, это, то, ,, что, это, очень, необычный, парк, США, .>,\n",
       " TokenList<В, этом, парке, располагаются, 2, самых, активных, вулкана, в, мире, -, Килауэа, и, Мауна, -, Лоа, .>,\n",
       " TokenList<В, этом, парке, всегда, все, по, -, новому, .>,\n",
       " TokenList<Из, -, за, вулканических, извержений, тут, постоянно, меняется, пейзаж, .>,\n",
       " TokenList<Наверное, ,, только, в, этом, месте, понимаешь, ,, что, такое, природа, ,, а, что, такое, человек, .>]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Мы видим, что токенизация в данной библиотеке происходит по другим правилам, поэтому будем использовать заранее токенизированный текст:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "list_parsed_tokens = []\n",
    "for token in list_tokens:\n",
    "    tagged_token = rus_pipeline.process(token)\n",
    "    token_info = conllu.parse(tagged_token)\n",
    "    list_parsed_tokens.append(token_info)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "dict_udpipe_tags = {}\n",
    "count = 0\n",
    "for token in list_parsed_tokens:\n",
    "    token_dict = token[0][0]\n",
    "    dict_udpipe_tags[count] = [token_dict['form'], token_dict['upos'], token_dict['feats']]\n",
    "    count += 1\n",
    "\n",
    "print(len(dict_udpipe_tags))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``stanza``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# загружаем модель для русского\n",
    "stanza.download('ru')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cryptography/hazmat/backends/openssl/x509.py:15: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 3.86MB/s]                    \n",
      "2021-10-03 14:57:22 INFO: Downloading default packages for language: ru (Russian)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.2/ru/default.zip: 100%|██████████| 574M/574M [15:22<00:00, 622kB/s]\n",
      "2021-10-03 15:12:55 INFO: Finished downloading models and saved to /home/yanina/stanza_resources.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "``stanza`` требует для токенизатор для теггинга"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos')  # mwt для русского нет"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-03 15:14:18 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "=========================\n",
      "\n",
      "2021-10-03 15:14:18 INFO: Use device: cpu\n",
      "2021-10-03 15:14:18 INFO: Loading: tokenize\n",
      "2021-10-03 15:14:18 INFO: Loading: pos\n",
      "2021-10-03 15:14:19 INFO: Done loading processors!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим, как токенизируется полный текст:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "doc = nlp(text)\n",
    "list_tokens_stanza = [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "print(list_tokens_stanza)\n",
    "print(len(list_tokens_stanza))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Первое', 'что', 'хочется', 'сказать', ',', 'так', 'это', 'то', ',', 'что', 'это', 'очень', 'необычный', 'парк', 'США', '.', 'В', 'этом', 'парке', 'располагаются', '2', 'самых', 'активных', 'вулкана', 'в', 'мире', '-', 'Килауэа', 'и', 'Мауна-Лоа', '.', 'В', 'этом', 'парке', 'всегда', 'все', 'по-новому', '.', 'Из-за', 'вулканических', 'извержений', 'тут', 'постоянно', 'меняется', 'пейзаж', '.', 'Наверное', ',', 'только', 'в', 'этом', 'месте', 'понимаешь', ',', 'что', 'такое', 'природа', ',', 'а', 'что', 'такое', 'человек', '.', 'Несмотря', 'на', 'свою', 'вулканическую', 'смертоносность', ',', 'тут', 'проживает', 'много', 'очень', 'редких', 'птиц', ',', 'которые', 'обитают', 'на', 'уникальных', 'лесах', '.', 'Также', 'только', 'в', 'этом', 'месте', 'вы', 'сможете', 'увидеть', 'по-настоящему', 'огромные', 'папоротники', '.', 'Создали', 'этот', 'удивительный', 'парк', 'в', '1916', 'г.', 'на', 'острове', 'Гавайи', ',', 'на', 'самом', 'крупном', 'из', 'Гавайских', 'островов', '.', 'Самым', 'главным', 'вулканом', 'парка', 'считают', 'вулкан', 'Килауэла', '.', 'Также', 'его', 'считают', 'святым', 'местом', 'и', 'домом', 'богини', 'вулкана', 'Пеле', '.', 'Раньше', 'гавайцы', 'ходили', 'к', 'кратеру', ',', 'чтобы', 'отдать', 'дары', 'богине', '.', 'Удивление', 'и', 'благоговение', 'в', 'мгновение', 'поднятия', 'заставляет', 'людей', 'безмолвствовать', ',', 'и', ',', 'словно', 'статуи', ',', 'они', 'застывают', 'на', 'одном', 'месте', '.', 'Кальдера', 'Килауэлы', 'очень', 'похожа', 'на', 'озеро', 'кипящей', 'огненной', 'жижи', 'площадью', '4,5', 'кв.м', 'и', 'глубиной', 'выше', '230', 'метров', '.', 'Кстати', ',', 'Килауэла', 'очень', 'активный', 'вулкан', '.', 'Он', 'не', 'может', 'успокоиться', 'еще', 'с', '1983', 'года', '!', 'На', 'краю', 'той', 'кальдеры', 'Томас', 'Джеггер', 'устроил', 'музей', '.', 'Экспонаты', 'там', 'всё', ',', 'что', 'имеет', 'отношение', 'к', 'вулканам', '–', 'одежда', ',', 'научное', 'оборудование', ',', 'используемая', 'вулканологами', 'и', 'так', 'далее', '.', 'Из', 'окон', 'этого', 'музея', 'вы', 'увидите', 'чудесный', 'вид', 'на', 'кальдеру', 'Калауэлы', ',', 'а', 'также', 'на', 'кратер', 'Галемаумау', '.', 'Также', 'в', 'парке', 'можно', 'увидеть', 'результаты', 'сотен', 'тысяч', 'лет', 'вулканической', 'работы', '.', 'Красивый', 'парк', 'еще', 'и', 'потому', ',', 'что', 'он', 'охватывает', 'все', 'высоты', '–', 'начиная', 'уровнем', 'моря', ',', 'заканчивая', 'вершиной', 'острова', '–', 'вулкана', 'Мауна', 'Лоа', '.', 'Наверное', ',', 'только', 'тут', 'путешественники', 'имеют', 'поистине', 'уникальную', 'возможность', 'увидеть', 'своими', 'глазами', 'как', 'спящие', 'вулканы', ',', 'чьи', 'склоны', 'уже', 'покрыты', 'тропическими', 'лесами', ',', 'так', 'и', 'достаточно', 'активные', ',', 'курящиеся', 'кратеры', '.', 'Темная', ',', 'застывшая', 'вулканическая', 'лава', 'большой', 'черной', 'рекой', ',', 'словно', 'змея', 'сползает', 'в', 'океан', ',', 'преграждая', 'все', 'дороги', ',', 'и', 'образуя', 'причудливую', 'береговую', 'линию', '.', 'В', 'том', 'месте', ',', 'где', 'лава', 'попадает', 'в', 'океан', ',', 'в', 'воздух', 'поднимается', 'пар', 'и', 'образуются', 'красивые', 'арки', '.', 'Кажется', ',', 'как', 'будто', 'ты', 'стоишь', 'на', 'краю', 'земли']\n",
      "365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нам повезло: ``stanza`` имеет ту же (или похожую) систему токенизации текста. Тогда попробуем достать теги:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "dict_stanza_tags = {}\n",
    "count = 0\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        dict_stanza_tags[count] = [word.text, word.upos, {word.feats if word.feats else \"_\"}]\n",
    "        count += 1\n",
    "\n",
    "print(len(dict_stanza_tags))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``pymorphy2``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "morph = MorphAnalyzer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "# будем использовать самый вероятный разбор\n",
    "dict_pymorphy_tags = {}\n",
    "count = 0\n",
    "for token in list_tokens:\n",
    "    parse = morph.parse(token)[0]\n",
    "    word = parse.word\n",
    "    pos_tag = parse.tag.POS\n",
    "    feats = parse.tag\n",
    "    lemma = parse.normal_form\n",
    "    if pos_tag == None:\n",
    "        for tag in ['LATN', 'PNCT', 'NUMB', 'ROMN', 'UNKN']:\n",
    "            if tag in feats:\n",
    "                dict_pymorphy_tags[count] = [word, tag, feats, lemma]\n",
    "    else:\n",
    "        for tag in ['NOUN', 'ADJF', 'ADJS',\n",
    "        'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS',\n",
    "        'GRND', 'NUMR', 'ADVB', 'NPRO',\n",
    "        'PRED', 'PREP', 'CONJ', 'PRCL', 'INTJ']:\n",
    "            if tag in feats:\n",
    "                dict_pymorphy_tags[count] = [word, tag, feats, lemma]\n",
    "    count += 1\n",
    "\n",
    "print(len(dict_pymorphy_tags))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разметка ``pymorphy2`` не соответствует разметке, которую мы приняли по умолчанию. Напишем код, который мог бы поменять теги ``pymorphy2`` на UD. Сначала создадим словарь однозначных соответствий тегов."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "dict_conf_pos = {\n",
    "    'ADJ': ['ADJF', 'ADJS', 'COMP', 'PRTF'],\n",
    "    'VERB': ['INFN', 'PRTS', 'GRND'],\n",
    "    'ADV': ['ADVB', 'PRED'], # есть сложности?\n",
    "    'PRON': ['NPRO'],  # есть сложности\n",
    "    'ADP': ['PREP']\n",
    "}\n",
    "dict_conf_tags = {\n",
    "    'NUM': ['NUMB'],  # есть сложности\n",
    "    'PUNCT': ['PNCT'],\n",
    "    'X': ['UNKN', 'ROMN', 'LATN'],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Словарь однозначных соответствий получился не таким большим. Рассмотрим каверзные моменты:\n",
    "* В изначальной системе есть только тег существительного. На деле система отличает и именованные сущности. Таким образом, если в морфологическом разборе есть 'Name sing'|'Name plur' - это имя собственное.\n",
    "* Проблема с числительными заключается в том, что по UD некоторые числительные вроде сотня, тысяча имеют в разных ситуациях разные теги. В частности, если перед таким числительным-существительным стоит реальное число, то оно также получает тег NUM, в остальных случаях - это существительное.\n",
    "* Неясно, что разбирается как предикатив (PRED) данной системой. Из примера понятно, что это могут быть адвербиалы. Вообще класс предикативов может включать в себя и другие части речи: прилагательные (депиктивы вроде *пришёл счастливым*), существительные (*стало лень*), причастия (краткие вроде *был прикован*). Исходя из эмпирических данных (см.ниже), предположим, что система умеет разбирать только случаи с адвербиалами.\n",
    "* Анализатор не различает два вида союзов. Придётся прибегнуть к словарному методу: если что-то анализируемое есть в некотором списке, то это сочинительный союз.\n",
    "* Другая ситуация с подчинительными союзами. Система UD отличает местоимения от простых подчинительных. Непонятно также, будет ли система оценивать подчинительные союзы как NPRO. В этом случае придумать правило достаточно сложно. \n",
    "* PRCL - это частица. Таким тегом разбираются стандартные частицы в русском, но не тот разбор, который предлагает UD. С точки зрения Universal Dependencies такие вещи разбиваются на 2 составляющие: AUX - вспомогательный глагол или частица, определяющие время или наклонение, и собственно PART. В принципе, можно прописать, что \"бы\", например, будет относиться к AUX.\n",
    "* Кроме того, проблема существует и с глаголами в связи с делением на VERB и AUX. Глагол *быть* может выступать показателем будущего времени. Здесь можно придумать правило про n-граммы, состоящие из двух глаголов, среди которых будет глагол *быть*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "list_str = ['мне было лень это делать',\n",
    "'мне стало плохо',\n",
    "'он пришёл счастливым ко мне',\n",
    "'мне было некогда ходить по магазинам']\n",
    "list_parsed_sents = []\n",
    "\n",
    "for str in list_str:\n",
    "    list_parsed_tokens = []\n",
    "    tokens = word_tokenize(str)\n",
    "    for token in tokens:\n",
    "        list_parsed_tokens.append(morph.parse(token))\n",
    "    list_parsed_sents.append(list_parsed_tokens)\n",
    "\n",
    "print([sent[2] for sent in list_parsed_sents])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[Parse(word='лень', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='лень', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'лень', 13, 0),)), Parse(word='лень', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='лень', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'лень', 13, 3),)), Parse(word='лень', tag=OpencorporaTag('PRED,pres'), normal_form='лень', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'лень', 242, 0),)), Parse(word='лёнь', tag=OpencorporaTag('NOUN,anim,masc,Name sing,voct,Infr'), normal_form='лёня', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'лёнь', 407, 7),)), Parse(word='лёнь', tag=OpencorporaTag('NOUN,anim,masc,Name plur,gent'), normal_form='лёня', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'лёнь', 407, 9),)), Parse(word='лёнь', tag=OpencorporaTag('NOUN,anim,masc,Name plur,accs'), normal_form='лёня', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'лёнь', 407, 11),))], [Parse(word='плохо', tag=OpencorporaTag('PRED,pres'), normal_form='плохо', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'плохо', 242, 0),)), Parse(word='плохо', tag=OpencorporaTag('ADVB,Prdx'), normal_form='плохо', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'плохо', 385, 0),)), Parse(word='плохо', tag=OpencorporaTag('ADJS,Qual neut,sing'), normal_form='плохой', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'плохо', 2376, 29),))], [Parse(word='счастливым', tag=OpencorporaTag('ADJF,Qual masc,sing,ablt'), normal_form='счастливый', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'счастливым', 405, 5),)), Parse(word='счастливым', tag=OpencorporaTag('ADJF,Qual neut,sing,ablt'), normal_form='счастливый', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'счастливым', 405, 18),)), Parse(word='счастливым', tag=OpencorporaTag('ADJF,Qual plur,datv'), normal_form='счастливый', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'счастливым', 405, 22),))], [Parse(word='некогда', tag=OpencorporaTag('PRED,pres'), normal_form='некогда', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'некогда', 242, 0),))]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Несмотря на то, что таких каверзных моментов много, в некоторых на самом деле неясно, проблема ли это разметки или действительно системы. Иногда кажется, что такими правилами мы фактически решаем омонимию за систему. В данной работе мы пропишем эти правила, проверяя также, в принципе распознала ли система камень преткновения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "list_conf_pymorphy_tags = [tag for value in dict_conf_pos.values() for tag in value ]\n",
    "print(list_conf_pymorphy_tags)\n",
    "list_conf_unusual_tags = [tag for value in dict_conf_tags.values() for tag in value]\n",
    "print(list_conf_unusual_tags)\n",
    "quantifiers = ['тысяча', 'миллион', 'миллиард', 'триллион']  # взяты со страницы UD\n",
    "list_values = [value for value in dict_pymorphy_tags.values()]\n",
    "# print(list_values)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ADJF', 'ADJS', 'COMP', 'PRTF', 'INFN', 'PRTS', 'GRND', 'ADVB', 'PRED', 'NPRO', 'PREP']\n",
      "['NUMB', 'PNCT', 'UNKN', 'ROMN', 'LATN']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "new_dict_pymorphy_tags = {}\n",
    "count = 0\n",
    "\n",
    "for ls in list_values:  # первый этап - меняем тривиальные вещи\n",
    "    if ls[1] in list_conf_pymorphy_tags or ls[1] in list_conf_unusual_tags:\n",
    "        for key, value in dict_conf.items():\n",
    "            if ls[1] in value:\n",
    "                ls[1] = key\n",
    "        # print('Выход', ls[1])\n",
    "        new_dict_pymorphy_tags[count] = ls\n",
    "    else:\n",
    "        new_dict_pymorphy_tags[count] = ls\n",
    "    count += 1\n",
    "\n",
    "# print(new_dict_pymorphy_tags)\n",
    "\n",
    "full_dict_pymorphy_tags = {}\n",
    "\n",
    "new_count = 0\n",
    "new_values = [value for value in new_dict_pymorphy_tags.values()]\n",
    "for ls in new_values:  # меняем нетривиальные вещи\n",
    "    if ls[1] == 'NOUN':  # именованные сущности\n",
    "        if 'Name' in ls[2] or 'Geox' in ls[2]:\n",
    "            ls[1] = 'PROPN'\n",
    "    if ls[1] == 'NUMR' and ls[3] in quantifiers:  # существительное или числительное\n",
    "        idx_list = new_count\n",
    "        idx_last_list = idx_list - 1\n",
    "        if new_values[idx_last_list][1] != 'NUM':\n",
    "            ls[1] = 'NOUN'\n",
    "    if ls[1] == 'CONJ':\n",
    "        if ls[3] in ['и', 'да', 'ни-ни', 'тоже', 'также',\n",
    "        'а', 'но', 'зато', 'однако', 'же',\n",
    "        'или', 'либо', 'то-то']:\n",
    "            ls[1] = 'CCONJ'\n",
    "        else:\n",
    "            ls[1] = 'SCONJ'\n",
    "    if ls[1] == 'PRCL':\n",
    "        if ls[3] == 'бы':\n",
    "            ls[1] = 'AUX'\n",
    "        else:\n",
    "            ls[1] = 'PART'\n",
    "    if ls[1] == 'VERB' and ls[3] == 'быть' and 'futr' in ls[2]:\n",
    "        idx_list = new_count\n",
    "        idx_next_list = idx_list + 1\n",
    "        if new_values[idx_last_list][1] == 'VERB':\n",
    "            ls[1] = 'AUX'\n",
    "    full_dict_pymorphy_tags[new_count] = ls\n",
    "\n",
    "    new_count += 1\n",
    "\n",
    "print(len(full_dict_pymorphy_tags))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Сравнение и оценка ответов систем"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "with open('corpora.csv', encoding='utf-8') as corpora_csv:\n",
    "    text = corpora_csv.read()\n",
    "\n",
    "text = text.replace(',\",', ',\";')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "text = text.replace('\",\"', 'comma')\n",
    "text = text.replace(',', ';')\n",
    "text = text.replace('comma', ',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "dict_standard_tag = {}\n",
    "splited_text = text.split('\\n')\n",
    "for line in splited_text:\n",
    "    if splited_text.index(line) != 0:\n",
    "        ls_line = line.split(';')\n",
    "        dict_standard_tag[ls_line[0]] = ls_line[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "def get_accuracy(system, standard):\n",
    "    \"\"\"\n",
    "    Функция, подсчитывающая accuracy для систем\n",
    "    \"\"\"\n",
    "\n",
    "    dict_2_sys = {}\n",
    "\n",
    "    count = 0\n",
    "    for my_tag in standard.values():\n",
    "        sys_tag = system[count][1]\n",
    "        dict_2_sys[count] = [my_tag, sys_tag]\n",
    "        count += 1\n",
    "\n",
    "    count_true = 0\n",
    "    for value_pair in dict_2_sys.values():\n",
    "        # print(value_pair)\n",
    "        if value_pair[0] == value_pair[1]:\n",
    "            count_true += 1\n",
    "\n",
    "    accuracy = count_true / len(dict_2_sys)\n",
    "\n",
    "    return accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "print('pymorphy', get_accuracy(full_dict_pymorphy_tags, dict_standard_tag))\n",
    "print('udpipe', get_accuracy(dict_udpipe_tags, dict_standard_tag))\n",
    "print('stanza', get_accuracy(dict_stanza_tags, dict_standard_tag))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pymorphy 0.1589958158995816\n",
      "udpipe 0.14644351464435146\n",
      "stanza 0.16736401673640167\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Лучше всего по нашим расчётам справилась библиотека ``stanza``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## О предыдущем домашнем задании"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Я могла бы предложить такие сочетания:\n",
    "* PART + VERB (*не люблю*)\n",
    "* PART + NOUN (*без надобности*, *к лицу*)\n",
    "* PRON + ADV (*всё отлично*)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}